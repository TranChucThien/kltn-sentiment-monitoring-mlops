{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa09f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/thientran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/thientran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime\n",
    "from utils.s3_process import read_csv_from_s3, push_csv_to_s3, upload_file_to_s3\n",
    "from utils.clean_text import preprocess\n",
    "from utils.s3_process import read_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46c5d1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data from S3\n",
    "with open(\"configs/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "with open(\"configs/secrets.yaml\", \"r\") as f:\n",
    "    config_secret = yaml.safe_load(f)\n",
    "\n",
    "bucket = config['s3']['bucket']\n",
    "raw_key = config['s3']['keys']['raw_data']\n",
    "validate_key = config['s3']['keys']['validation_data']\n",
    "train_path = f\"s3a://{bucket}/{raw_key}\"\n",
    "test_path = f\"s3a://{bucket}/{validate_key}\"\n",
    "\n",
    "AWS_KEY_PATH = config['aws']['access_key_path']\n",
    "AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY = read_key(config_secret['aws']['access_key_path'])\n",
    "AWS_REGION = config['aws']['region']\n",
    "S3_OUTPUT_KEY = config['s3']['keys']['dataset']\n",
    "BUCKET_NAME = config['s3']['bucket']\n",
    "\n",
    "# Read CSV file from S3 (raw data)\n",
    "data_train = read_csv_from_s3(train_path, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION)\n",
    "data_test = read_csv_from_s3(test_path, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57926665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|    Product|   Label|                Text|\n",
      "+-----------+--------+--------------------+\n",
      "|Borderlands|Positive|I am coming to th...|\n",
      "|Borderlands|Positive|im getting on bor...|\n",
      "|Borderlands|Positive|im coming on bord...|\n",
      "|Borderlands|Positive|im getting on bor...|\n",
      "|Borderlands|Positive|im getting into b...|\n",
      "+-----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+--------------------+\n",
      "|  ID|  Product|     Label|                Text|\n",
      "+----+---------+----------+--------------------+\n",
      "|3364| Facebook|Irrelevant|I mentioned on Fa...|\n",
      "| 352|   Amazon|   Neutral|BBC News - Amazon...|\n",
      "|8312|Microsoft|  Negative|@Microsoft Why do...|\n",
      "|4371|    CS-GO|  Negative|CSGO matchmaking ...|\n",
      "|4433|   Google|   Neutral|Now the President...|\n",
      "+----+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_train.show(5)\n",
    "\n",
    "data_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5176f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape của data_train: (hàng=74681, cột=3)\n",
      "Shape của data_test: (hàng=1510, cột=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(data_train))\n",
    "print(type(data_test))\n",
    "\n",
    "print(f\"Shape của data_train: (hàng={data_train.count()}, cột={len(data_train.columns)})\")\n",
    "print(f\"Shape của data_test: (hàng={data_test.count()}, cột={len(data_test.columns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7968578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+\n",
      "|Product|Label|Text|\n",
      "+-------+-----+----+\n",
      "|      0|    0| 686|\n",
      "+-------+-----+----+\n",
      "\n",
      "+---+-------+-----+----+\n",
      "| ID|Product|Label|Text|\n",
      "+---+-------+-----+----+\n",
      "|  0|    467|  495| 510|\n",
      "+---+-------+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, sum, when\n",
    "\n",
    "# # Kiểm tra giá trị null cho từng cột trong data_train\n",
    "# null_counts_train = data_train.agg(*[sum(when(isnull(col), 1).otherwise(0)).alias(col) for col in data_train.columns])\n",
    "# null_counts_train.show()\n",
    "\n",
    "# # Kiểm tra giá trị null cho từng cột trong data_test\n",
    "# null_counts_test = data_test.agg(*[sum(when(isnull(col), 1).otherwise(0)).alias(col) for col in data_test.columns])\n",
    "# null_counts_test.show()\n",
    "def check_null_values(data):\n",
    "    null_counts = data.agg(*[sum(when(isnull(col), 1).otherwise(0)).alias(col) for col in data.columns])\n",
    "    return null_counts\n",
    "check_null_values(data_train).show()\n",
    "check_null_values(data_test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b671528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+\n",
      "|Product|Label|Text|\n",
      "+-------+-----+----+\n",
      "|      0|    0|   0|\n",
      "+-------+-----+----+\n",
      "\n",
      "+---+-------+-----+----+\n",
      "| ID|Product|Label|Text|\n",
      "+---+-------+-----+----+\n",
      "|  0|      0|    0|   0|\n",
      "+---+-------+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_test = data_test.dropna()\n",
    "data_train = data_train.dropna()\n",
    "check_null_values(data_train).show()\n",
    "check_null_values(data_test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "066b261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+--------------------+--------------------+\n",
      "|  ID|  Product|     Label|                Text|      processed_text|\n",
      "+----+---------+----------+--------------------+--------------------+\n",
      "|3364| Facebook|Irrelevant|I mentioned on Fa...|i mentioned on fa...|\n",
      "| 352|   Amazon|   Neutral|BBC News - Amazon...|bbc news - amazon...|\n",
      "|8312|Microsoft|  Negative|@Microsoft Why do...|@microsoft why do...|\n",
      "|4371|    CS-GO|  Negative|CSGO matchmaking ...|csgo matchmaking ...|\n",
      "|4433|   Google|   Neutral|Now the President...|now the president...|\n",
      "+----+---------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+--------------------+\n",
      "|    Product|   Label|                Text|      processed_text|\n",
      "+-----------+--------+--------------------+--------------------+\n",
      "|Borderlands|Positive|I am coming to th...|i am coming to th...|\n",
      "|Borderlands|Positive|im getting on bor...|im getting on bor...|\n",
      "|Borderlands|Positive|im coming on bord...|im coming on bord...|\n",
      "|Borderlands|Positive|im getting on bor...|im getting on bor...|\n",
      "|Borderlands|Positive|im getting into b...|im getting into b...|\n",
      "+-----------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "\n",
    "def lower_case(data):\n",
    "    return data.withColumn('processed_text', lower(data['Text']))\n",
    "data_train = lower_case(data_train)\n",
    "data_test = lower_case(data_test)\n",
    "data_test.show(5)\n",
    "data_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b38c75ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+--------------------+\n",
      "|    Product|   Label|                Text|      processed_text|\n",
      "+-----------+--------+--------------------+--------------------+\n",
      "|Borderlands|Positive|I am coming to th...|i am coming to th...|\n",
      "|Borderlands|Positive|im getting on bor...|im getting on bor...|\n",
      "|Borderlands|Positive|im coming on bord...|im coming on bord...|\n",
      "|Borderlands|Positive|im getting on bor...|im getting on bor...|\n",
      "|Borderlands|Positive|im getting into b...|im getting into b...|\n",
      "+-----------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+--------------------+--------------------+\n",
      "|  ID|  Product|     Label|                Text|      processed_text|\n",
      "+----+---------+----------+--------------------+--------------------+\n",
      "|3364| Facebook|Irrelevant|I mentioned on Fa...|i mentioned on fa...|\n",
      "| 352|   Amazon|   Neutral|BBC News - Amazon...|bbc news - amazon...|\n",
      "|8312|Microsoft|  Negative|@Microsoft Why do...|@microsoft why do...|\n",
      "|4371|    CS-GO|  Negative|CSGO matchmaking ...|csgo matchmaking ...|\n",
      "|4433|   Google|   Neutral|Now the President...|now the president...|\n",
      "+----+---------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# remove html tags\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "def remove_html_tags(data):\n",
    "    return data.withColumn('processed_text', regexp_replace(data['processed_text'], \"<.*?>\", \"\"))\n",
    "\n",
    "data_train = remove_html_tags(data_train)\n",
    "data_test = remove_html_tags(data_test)\n",
    "data_train.show(5)\n",
    "data_test.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05d3b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, regexp_replace, trim, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, IDF, StringIndexer\n",
    "# Khởi tạo SparkSession (nếu chưa có)\n",
    "spark = SparkSession.builder.appName(\"TweetProcessing\").getOrCreate()\n",
    "\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\",\n",
    "    \"L8\": \"Late\",\n",
    "    \"SMH\": \"Shaking My Head\",\n",
    "    \"YOLO\": \"You Only Live Once\",\n",
    "    \"TLDR\": \"Too Long; Didn't Read\",\n",
    "    \"FOMO\": \"Fear Of Missing Out\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"BFFL\": \"Best Friends For Life\",\n",
    "    \"TMI\": \"Too Much Information\",\n",
    "    \"DM\": \"Direct Message\",\n",
    "    \"STFU\": \"Shut The F... Up\",\n",
    "    \"WTH\": \"What The Heck\",\n",
    "    \"LMAOROTF\": \"Laughing My A... Off Rolling On The Floor\",\n",
    "    \"PPL\": \"People\",\n",
    "    \"SFLR\": \"Sorry For Late Reply\",\n",
    "    \"G2G\": \"Got To Go\",\n",
    "    \"S2R\": \"Send To Receive\"\n",
    "}\n",
    "# Broadcast variables (optional for local mode, but good practice for scalability)\n",
    "punctuation_broadcast = spark.sparkContext.broadcast(punctuation)\n",
    "stopwords_broadcast = spark.sparkContext.broadcast(set(stopwords.words(\"english\")))\n",
    "lemmatizer_broadcast = spark.sparkContext.broadcast(WordNetLemmatizer())\n",
    "chat_words_broadcast = spark.sparkContext.broadcast(chat_words) # Assuming chat_words dictionary is defined\n",
    "\n",
    "# Define UDFs for text processing\n",
    "def remove_punct(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", punctuation_broadcast.value))\n",
    "remove_punct_udf = udf(remove_punct, StringType())\n",
    "\n",
    "def remove_sw(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords_broadcast.value])\n",
    "remove_sw_udf = udf(remove_sw, StringType())\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer_broadcast.value.lemmatize(word) for word in word_tokenize(text)])\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "\n",
    "def replace_abbrev(text):\n",
    "    return \" \".join([chat_words_broadcast.value.get(word.upper(), word) for word in text.split()])\n",
    "replace_abbrev_udf = udf(replace_abbrev, StringType())\n",
    "\n",
    "def remove_urls_spark_udf(text):\n",
    "    url_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    return regexp_replace(text, url_pattern, \"\")\n",
    "remove_urls_spark = udf(remove_urls_spark_udf, StringType())\n",
    "\n",
    "def remove_emojis_spark_udf(text):\n",
    "    emoji_pattern = (\n",
    "        \"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]+\"\n",
    "    )\n",
    "    return regexp_replace(text, emoji_pattern, \"\")\n",
    "remove_emojis_spark = udf(remove_emojis_spark_udf, StringType())\n",
    "\n",
    "# Define the preprocessing pipeline for a single column\n",
    "def preprocess_text_spark(df, input_column='Text', output_column='processed_text'):\n",
    "    df_processed = df.withColumn(output_column, lower(df[input_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, regexp_replace(df_processed[output_column], \"<.*?>\", \"\"))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_urls_spark(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_emojis_spark(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, replace_abbrev_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_punct_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_sw_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, trim(regexp_replace(df_processed[output_column], r\"\\s+\", \" \")))\n",
    "    df_processed = df_processed.withColumn(output_column, lemmatize_udf(df_processed[output_column]))\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d91398a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 18:37:01 ERROR Executor: Exception in task 0.0 in stage 110.0 (TID 118)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_124265/367021442.py\", line 23, in remove_html_tags_func\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9642, in regexp_replace\n",
      "    pattern_col = _create_column_from_literal(pattern)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 50, in _create_column_from_literal\n",
      "    sc = get_active_spark_context()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/07 18:37:01 WARN TaskSetManager: Lost task 0.0 in stage 110.0 (TID 118) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_124265/367021442.py\", line 23, in remove_html_tags_func\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9642, in regexp_replace\n",
      "    pattern_col = _create_column_from_literal(pattern)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 50, in _create_column_from_literal\n",
      "    sc = get_active_spark_context()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/07 18:37:01 ERROR TaskSetManager: Task 0 in stage 110.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_124265/367021442.py\", line 23, in remove_html_tags_func\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9642, in regexp_replace\n    pattern_col = _create_column_from_literal(pattern)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 50, in _create_column_from_literal\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m processed_train_df = preprocess_pipeline_spark(data_train, input_column=\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m, output_column=\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    109\u001b[39m processed_test_df = preprocess_pipeline_spark(data_test, input_column=\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m, output_column=\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mprocessed_train_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mText\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessed_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m processed_test_df.select(\u001b[33m\"\u001b[39m\u001b[33mText\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m\"\u001b[39m).show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mPythonException\u001b[39m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_124265/367021442.py\", line 23, in remove_html_tags_func\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9642, in regexp_replace\n    pattern_col = _create_column_from_literal(pattern)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 50, in _create_column_from_literal\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/thientran/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, regexp_replace, trim, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Khởi tạo SparkSession (nếu chưa có)\n",
    "spark = SparkSession.builder.appName(\"TweetProcessing\").getOrCreate()\n",
    "\n",
    "# Broadcast variables (optional for local mode)\n",
    "punctuation_broadcast = spark.sparkContext.broadcast(punctuation)\n",
    "stopwords_broadcast = spark.sparkContext.broadcast(set(stopwords.words(\"english\")))\n",
    "lemmatizer_broadcast = spark.sparkContext.broadcast(WordNetLemmatizer())\n",
    "chat_words_broadcast = spark.sparkContext.broadcast(chat_words) # Giả sử chat_words đã được định nghĩa\n",
    "\n",
    "# Define regular Python functions for UDFs\n",
    "def lower_func(x):\n",
    "    return x.lower() if x else None\n",
    "\n",
    "def remove_html_tags_func(x):\n",
    "    return regexp_replace(x, \"<.*?>\", \"\") if x else None\n",
    "\n",
    "def remove_urls_func(x):\n",
    "    return regexp_replace(x, r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\") if x else None\n",
    "\n",
    "def remove_punctuations_func(x):\n",
    "    return x.translate(str.maketrans(\"\", \"\", punctuation_broadcast.value)) if x else None\n",
    "\n",
    "def apply_chat_word_map_func(text):\n",
    "    if text:\n",
    "        return \" \".join([chat_words_broadcast.value.get(word.upper(), word) for word in text.split()])\n",
    "    return None\n",
    "\n",
    "def remove_stopwords_func(text):\n",
    "    if text:\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stopwords_broadcast.value]\n",
    "        return \" \".join(filtered_words)\n",
    "    return None\n",
    "\n",
    "def remove_emojis_func(text):\n",
    "    if text:\n",
    "        emoji_pattern = (\n",
    "            \"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "            \"\\U0001F900-\\U0001F99F\"  # Supplemental Symbols and Pictographs\n",
    "            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "            \"]+\"\n",
    "        )\n",
    "        return regexp_replace(text, emoji_pattern, \"\")\n",
    "    return None\n",
    "\n",
    "def remove_extra_whitespaces_func(x):\n",
    "    return trim(regexp_replace(x, r\"\\s+\", \" \")) if x else None\n",
    "\n",
    "def lemmatize_text_func(text):\n",
    "    if text:\n",
    "        lemmatizer = lemmatizer_broadcast.value\n",
    "        words = word_tokenize(text)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        return \" \".join(lemmatized_words)\n",
    "    return None\n",
    "\n",
    "# Create UDFs from the regular Python functions\n",
    "convert_to_lowercase_spark_udf = udf(lower_func, StringType())\n",
    "remove_html_tags_spark_udf = udf(remove_html_tags_func, StringType())\n",
    "remove_urls_spark_udf = udf(remove_urls_func, StringType())\n",
    "remove_punctuations_spark_udf = udf(remove_punctuations_func, StringType())\n",
    "apply_chat_word_map_spark_udf = udf(apply_chat_word_map_func, StringType())\n",
    "remove_stopwords_spark_udf = udf(remove_stopwords_func, StringType())\n",
    "remove_emojis_spark_udf = udf(remove_emojis_func, StringType())\n",
    "remove_extra_whitespaces_spark_udf = udf(remove_extra_whitespaces_func, StringType())\n",
    "lemmatize_text_spark_udf = udf(lemmatize_text_func, StringType())\n",
    "\n",
    "def preprocess_pipeline_spark(df, input_column='Text', output_column='processed_text'):\n",
    "    \"\"\"\n",
    "    Applies a series of text preprocessing steps to a specified column of a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The input Spark DataFrame.\n",
    "        input_column: The name of the column containing the text.\n",
    "        output_column: The name of the new column to store the processed text.\n",
    "\n",
    "    Returns:\n",
    "        A new Spark DataFrame with the processed text in the specified output column.\n",
    "    \"\"\"\n",
    "    df_processed = df.withColumn(output_column, convert_to_lowercase_spark_udf(df[input_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_html_tags_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_urls_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_emojis_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, apply_chat_word_map_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_punctuations_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_stopwords_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, remove_extra_whitespaces_spark_udf(df_processed[output_column]))\n",
    "    df_processed = df_processed.withColumn(output_column, lemmatize_text_spark_udf(df_processed[output_column]))\n",
    "    return df_processed\n",
    "\n",
    "# Example of how to use the pipeline:\n",
    "processed_train_df = preprocess_pipeline_spark(data_train, input_column='Text', output_column='processed_text')\n",
    "processed_test_df = preprocess_pipeline_spark(data_test, input_column='Text', output_column='processed_text')\n",
    "processed_train_df.select(\"Text\", \"processed_text\").show(5)\n",
    "processed_test_df.select(\"Text\", \"processed_text\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
