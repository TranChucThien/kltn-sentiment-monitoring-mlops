import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, regexp_replace, trim, when, length
import yaml
import os
from pyspark.sql import SparkSession
import boto3
from botocore.exceptions import ClientError
import os
import shutil
from utils.clean_text import clean_text_column, preprocess

# def clean_text_column(df, input_col="Text", output_col="Text"):
#     """
#     Clean text in a DataFrame column using native Spark functions.

#     Args:
#         df (DataFrame): The input DataFrame.
#         input_col (str): The name of the column containing the raw text.
#         output_col (str): The name of the column where the cleaned text will be stored (can be the same as input_col to overwrite).

#     Returns:
#         DataFrame: DataFrame with the cleaned text column.
#     """
#     return (df.withColumn(output_col, regexp_replace(col(input_col), r'https?://\S+|www\.\S+|\.com\S+|youtu\.be/\S+', ''))  # Remove URLs
#               .withColumn(output_col, regexp_replace(col(output_col), r'(@|#)\w+', ''))  # Remove mentions and hashtags
#               .withColumn(output_col, lower(col(output_col)))  # Convert to lowercase
#               .withColumn(output_col, regexp_replace(col(output_col), r'[^a-zA-Z\s]', ''))  # Remove non-alphabetic characters
#               .withColumn(output_col, regexp_replace(col(output_col), r'\s+', ' '))  # Replace multiple spaces with a single space
#               )


# def preprocess(spark, config):
#     """
#     Preprocess the data including renaming columns, cleaning text, and handling labels.

#     Args:
#         spark (SparkSession): The Spark session.
#         config (dict): The configuration dictionary containing input path and other parameters.

#     Returns:
#         DataFrame: The preprocessed DataFrame.
#     """
#     # Read the CSV file into a DataFrame
#     data = spark.read.csv(config['input_path'], header=True, inferSchema=True)
#     print("Data change column name:")
#     data.show(3)

#     # Rename columns if necessary
#     data = data.withColumnRenamed("_c0", "Product")  # Rename column _c0 to Product
#     data = data.withColumnRenamed("_c1", "Label")  # Rename column _c1 to Label
#     data = data.withColumnRenamed("_c2", "Text")  # Rename column _c2 to Text
    
#     print("Data after renaming columns:")
#     data.show(3)
    
#     # Remove the header row if necessary (based on index)
#     data = data.rdd.zipWithIndex().filter(lambda row: row[1] != 0).map(lambda row: row[0]).toDF(["Product", "Label", "Text"])
    
#     # Drop rows where 'Text' column has null values
#     data = data.dropna(subset=['Text'])
    
#     # Remove rows where 'Text' or 'Label' columns are empty after trimming spaces
#     data = data.filter(trim(col("Text")) != "")
#     data = data.filter(trim(col("Label")) != "")
#     data.show(3)
    
#     # Drop the 'Product' column
#     data = data.drop("Product")
#     data.show(3)
    
#     # Clean the text in the 'Text' column
#     data = clean_text_column(data, input_col="Text", output_col="Text")
    
#     # Convert 'Label' values to numeric values (0 for Negative, 1 for Positive, 2 for Neutral, 3 for Irrelevant)
#     data = data.withColumn("Label",
#             when(col("Label") == "Negative", 0)
#           .when(col("Label") == "Positive", 1)
#           .when(col("Label") == "Neutral", 2)
#           .when(col("Label") == "Irrelevant", 3)
#           .otherwise(None)  # Fallback for unknown labels
#     )
    
#     # Remove rows where the 'Text' column is empty or 'Label' is null
#     data = data.filter(length(col("Text")) > 0)
#     data = data.withColumn("Text", trim(col("Text")))
#     data = data.filter((col("Text").isNotNull()) & (length(col("Text")) > 0))
#     data = data.filter(col("Label").isNotNull())
#     data = data.filter(col("Label").isNotNull() & (length(col("Text")) > 0))

    
#     data.show(3)
    
#     return data


if __name__ == "__main__":
    # Create SparkSession
    # spark = SparkSession.builder \
    #     .appName("Text Classification with PySpark") \
    #     .getOrCreate()

    # Read config from YAML file
    with open("configs/config.yaml", "r") as f:
        config = yaml.safe_load(f)
        
    print("üìÅ Input data path:", config['data']['input_path'])

    # Preprocess the data
    data = preprocess(config['data']['input_path'] )
    
    print("üìÑ Cleaned data:")
    data.show(20)
# L∆∞u DataFrame 'data' th√†nh m·ªôt file CSV duy nh·∫•t
    print(config['data']['dataset_output_path'])
    data.coalesce(1).write.option("header", "true").mode("overwrite").csv(config['data']['dataset_output_path'])
    # Stop Spark session
    # spark.stop()

    # save_data(data, config['dataset_output_path'], format="csv")
    
    
    
    
    
    
    
        # C·∫•u h√¨nh th√¥ng tin AWS (ƒë·∫£m b·∫£o b·∫°n ƒë√£ c·∫•u h√¨nh ch√∫ng)
    AWS_ACCESS_KEY_ID = "YOUR_ACCESS_KEY"
    AWS_SECRET_ACCESS_KEY = "YOUR_SECRET_KEY"
    AWS_REGION = "your-aws-region"
    BUCKET_NAME = "your-unique-bucket-name"
    S3_OUTPUT_KEY = "output_from_spark.csv"  # T√™n file CSV tr√™n S3


    AWS_ACCESS_KEY_ID = 
    AWS_SECRET_ACCESS_KEY = "
    AWS_REGION = "us-east-2"  # V√≠ d·ª•: "ap-southeast-1" (Singapore), "us-east-1" (N. Virginia)
    BUCKET_NAME = "tranchucthienops"  # Ch·ªçn m·ªôt t√™n bucket duy nh·∫•t tr√™n to√†n c·∫ßu
    CSV_FILE_PATH = "data/twitter_training.csv"
    S3_KEY_NAME = "output.csv"  # T√™n file b·∫°n mu·ªën l∆∞u tr√™n S3
    LOCAL_TEMP_DIR = "/tmp/spark_output"  
     # Kh·ªüi t·∫°o Spark Session
    spark = SparkSession.builder.appName("SparkUploadToS3") \
        .config("spark.local.dir", LOCAL_TEMP_DIR) \
        .getOrCreate()

    # C·∫•u h√¨nh AWS credentials cho Spark
    spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", AWS_ACCESS_KEY_ID)
    spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", AWS_SECRET_ACCESS_KEY)
    spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
    if AWS_REGION not in ['us-east-1', None]:
        spark._jsc.hadoopConfiguration().set(f"fs.s3a.endpoint.{AWS_REGION}.amazonaws.com", f"s3.{AWS_REGION}.amazonaws.com")

    # T·∫°o th∆∞ m·ª•c t·∫°m n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i
    os.makedirs(LOCAL_TEMP_DIR, exist_ok=True)

    try:
        # Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ DataFrame 'data' t·ª´ vi·ªác ƒë·ªçc file CSV
        # data = spark.read.csv(CSV_FILE_PATH, header=True, inferSchema=True)
        data.printSchema()
        data.show()

        # 1. L∆∞u DataFrame xu·ªëng file CSV trong WSL filesystem (t·∫°m th·ªùi)
        local_output_path = os.path.join(LOCAL_TEMP_DIR, "processed_data")
        output_uri = f"file:///{local_output_path}"

        data.write.csv(output_uri, header=True, mode="overwrite")

        # Spark l∆∞u th√†nh m·ªôt th∆∞ m·ª•c ch·ª©a nhi·ªÅu part file, ch√∫ng ta c·∫ßn t√¨m file .csv th·ª±c t·∫ø
        output_dir = local_output_path
        part_files = [f for f in os.listdir(output_dir) if f.startswith("part-") and f.endswith(".csv")]

        if part_files:
            # L·∫•y ƒë∆∞·ªùng d·∫´n c·ªßa file part ƒë·∫ßu ti√™n
            actual_output_file_path = os.path.join(output_dir, part_files[0])

            # 2. S·ª≠ d·ª•ng boto3 ƒë·ªÉ upload file CSV t·ª´ local l√™n S3
            s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)
            try:
                s3_client.upload_file(actual_output_file_path, BUCKET_NAME, S3_OUTPUT_KEY)
                print(f"ƒê√£ t·∫£i DataFrame l√™n s3://{BUCKET_NAME}/{S3_OUTPUT_KEY}")
            except FileNotFoundError:
                print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{actual_output_file_path}'.")
            except ClientError as e:
                print(f"L·ªói khi t·∫£i file l√™n S3: {e}")

            # 3. D·ªçn d·∫πp file v√† th∆∞ m·ª•c t·∫°m
            os.remove(actual_output_file_path)
            shutil.rmtree(local_output_path, ignore_errors=True)
        else:
            print("L·ªói: Kh√¥ng t√¨m th·∫•y file CSV part n√†o sau khi Spark ghi.")

    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói: {e}")

    finally:
        # D·ª´ng Spark Session
        spark.stop()